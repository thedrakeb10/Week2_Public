{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworks_Tutorial_part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedrakeb10/Week2_Public/blob/master/Notebooks/05_NeuralNetworks/NeuralNetworks_Tutorial_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlpG_fIiqdJU",
        "colab_type": "text"
      },
      "source": [
        "## Building a Simple Neural Network with Tensorflow Keras\n",
        "\n",
        "In this notebook we are going to walk through building a simple neural network to classify sequence data. This tutorial will be meant as a fast overview of building/training neural networks with Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFSzOvSduDf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ad127d3-ea48-49c9-84b5-e0394e17a063"
      },
      "source": [
        "# Import useful libraries\n",
        "\n",
        "#Needed for terminal functions (i.e. wget)\n",
        "import os\n",
        "\n",
        "#For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#For dataframe manipulation\n",
        "import pandas as pd\n",
        "\n",
        "#For data preprocessing\n",
        "from sklearn.preprocessing import StandardScaler #Use StandardScaler from scikitlearn\n",
        "from sklearn.utils import shuffle #Used to shuffle up examples before training\n",
        "\n",
        "#Keras-related imports\n",
        "from keras.models import Sequential  #we will build our models layer by layer\n",
        "from keras.layers import Dense  #we want to use dense layers in our model\n",
        "\n",
        "#Keras is built on top of tensorflow library\n",
        "import tensorflow as tf #tf has many helpful functions for training networks like loss functions, optimization methods, etc."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep_09mlyqa5G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "07dbd9f4-91c0-4de0-ab25-835d1a65710e"
      },
      "source": [
        "#Load the dataframe that contains all features calculated in the last notebook (only run once)\n",
        "os.system('wget https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week2/spoken_digit_manual_features.csv')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zChcGaqVysRB",
        "colab_type": "text"
      },
      "source": [
        "## Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVwsmOGvw7jp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "fc9c6ace-ce81-4c73-b9a6-d07ecad4da12"
      },
      "source": [
        "#Load dataframe and print its contents to jog memory\n",
        "spoken_df = pd.read_csv('spoken_digit_manual_features.csv', index_col = 0)\n",
        "print(spoken_df.head(10))\n",
        "print('\\n')\n",
        "\n",
        "#Check how many unique speakers exist in the dataset\n",
        "speakers=set(spoken_df['speaker'])\n",
        "print(f'There are {len(speakers)} unique speakers in the dataset')\n",
        "\n",
        "# Our goal for this is to build a neural network that learns to classify which\n",
        "# of 5 speakers is recorded in a sample based on the features:\n",
        "# spectral centroid, spectral flatness, and maximum frequency\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                file  digit   speaker  trial           SC        SF          MF\n",
            "0   5_yweweler_8.wav      5  yweweler      8  1029.497959  0.397336  745.878340\n",
            "1    3_george_49.wav      3    george      4  1881.296834  0.387050  323.943662\n",
            "2  9_yweweler_44.wav      9  yweweler      4  1093.951856  0.394981  244.648318\n",
            "3  8_yweweler_33.wav      8  yweweler      3  1409.543285  0.487496  392.350401\n",
            "4      7_theo_34.wav      7      theo      3   887.361601  0.396825  130.640309\n",
            "5   1_jackson_45.wav      1   jackson      4  1007.568129  0.324100  216.306156\n",
            "6  6_yweweler_18.wav      6  yweweler      1  1286.701352  0.498813  400.715564\n",
            "7    9_george_35.wav      9    george      3  1405.092061  0.353083  447.239693\n",
            "8   9_jackson_32.wav      9   jackson      3  1172.899961  0.477907  114.892780\n",
            "9    8_george_26.wav      8    george      2  1959.977577  0.462901  320.537966\n",
            "\n",
            "\n",
            "There are 5 unique speakers in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riycN8SdyxNT",
        "colab_type": "text"
      },
      "source": [
        "## Structure Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMp_z7W9vZV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the keras neural network\n",
        "\n",
        "#this allows us to add layers sequentially (i.e. first->last)\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "#create a first layer of 12 neurons, and a rectified linear unit activation function\n",
        "model.add(tf.keras.layers.Dense(8, input_shape=(3,), activation=tf.nn.relu)) #input dimension needs to be number of features\n",
        "\n",
        "#add two dense layers with 8 units each\n",
        "#(note that we don't need to specify input size because keras determines input size from previous layer)\n",
        "model.add(tf.keras.layers.Dense(8, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(8, activation=tf.nn.relu))\n",
        "\n",
        "# output dimension needs to be number of classes in order for each to get a score\n",
        "model.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax)) "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p9UDYX-y_v9",
        "colab_type": "text"
      },
      "source": [
        "## Specify a Loss Function and an Optimizer for NN Model\n",
        "\n",
        "Let's describe why each of these components is necessary, and how it is used in training a neural network.\n",
        "\n",
        "**Loss Function** - This is the quantity that should be minimized when the network is trained. (It is like the mean squared error for a linear regression.) A neural network can use squared error as a loss function, but there are also other options. In the case of a neural network trying to classify samples into 1 of n categories system a common choice is called cross entropy loss.\n",
        "\n",
        "**Optimizer** - When a neural network is trained, it changes weights in the network to minimize the loss function. The optimizer governs how the neural network iteratively changes its weights as it minimizes loss. Many optimizers use the derivative of the loss function with respect to all the weights to decide which direction to change network weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKRMZVOkyoUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify a loss function for our network\n",
        "\n",
        "#Note that the metrics input argument governs what will be reported as the network is trained \n",
        "model.compile(loss = tf.keras.losses.categorical_crossentropy, optimizer = tf.keras.optimizers.Adam(learning_rate=0.01) , metrics = ['accuracy'])\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgP9AW_KSDuy",
        "colab_type": "text"
      },
      "source": [
        "## Convert Labels into \"Onehot\" Vectors\n",
        "\n",
        "Predictions output by the model need to be compared to some truth label. Currently, the model predicts a 5-element vector of \"prediction values\" for every sample. The truth labels thus need to be converted to a 5-element vector with a 1 in the correct index and zeros in all others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix3lG15TSo42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make dictionary to convert from speaker names to indices\n",
        "name2int_dict = {name: ind for (ind, name) in enumerate(set(spoken_df['speaker']))}\n",
        "\n",
        "y_labels = spoken_df['speaker']\n",
        "#set y_labels to be indices of speaker\n",
        "y_labels = [name2int_dict[name] for name in y_labels]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uegmriZU29sA",
        "colab_type": "text"
      },
      "source": [
        "## Standardize Data and split into train/validation/test sets\n",
        "\n",
        "Scaling data is generally good practice before attempting to fit a model. Having inputs with large differences in scale can affect how the optimizer changes weights to minimize the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM-ON_x930fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#downselect to only the 3 columns of the dataset we are learning from\n",
        "X_data = spoken_df[['SC', 'SF', 'MF']].to_numpy()\n",
        "\n",
        "#Decide how large to make validation and test sets\n",
        "n_val = 250\n",
        "n_test = 250\n",
        "\n",
        "#Shuffle data before partitioning\n",
        "X_data, y_labels = shuffle(X_data, y_labels, random_state = 25)\n",
        "\n",
        "#Partition\n",
        "X_data_test, y_labels_test = X_data[:n_test,:], y_labels[:n_test]\n",
        "X_data_val, y_labels_val = X_data[n_test:n_test+n_val,:], y_labels[n_test:n_test+n_val]\n",
        "X_data_train, y_labels_train = X_data[n_test+n_val:,:], y_labels[n_test+n_val:]\n",
        "\n",
        "#Scale data\n",
        "scaler = StandardScaler()\n",
        "X_data_train=scaler.fit_transform(X_data_train)\n",
        "X_data_val = scaler.transform(X_data_val)\n",
        "X_data_test = scaler.transform(X_data_test)\n",
        "\n",
        "#convert labels to onehot\n",
        "y_labels_train = tf.keras.utils.to_categorical(y_labels_train, 5)\n",
        "y_labels_val =  tf.keras.utils.to_categorical(y_labels_val, 5)\n",
        "y_labels_test =  tf.keras.utils.to_categorical(y_labels_test, 5)\n",
        "\n",
        "training_set = tf.data.Dataset.from_tensor_slices((X_data_train, y_labels_train))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnDGqneBOsW_",
        "colab_type": "text"
      },
      "source": [
        "## Fit Model to Data, Specify Number of Epochs and Batch Size\n",
        "\n",
        "**Batch Size** - In each iteration of the optimizer, how many samples are taken into account when calculating derivatives of the loss function? (If batch size is less than number of samples, there will be multiple optimization iterations per epoch.)\n",
        "\n",
        "**Epochs** - How many times should the data be passed through before optimization is finished?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkeUHhxZ3iOF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87dd4357-9656-40f6-bc3d-059f5a9ae2b9"
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 100\n",
        "\n",
        "training_set = training_set.batch(batch_size) #set batch size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for signals, labels in training_set:\n",
        "        tr_loss, tr_accuracy = model.train_on_batch(signals, labels)\n",
        "    val_loss, val_accuracy = model.evaluate(X_data_val, y_labels_val)\n",
        "    print(('Epoch #%d\\t Training Loss: %.2f\\tTraining Accuracy: %.2f\\t'\n",
        "         'Validation Loss: %.2f\\tValidation Accuracy: %.2f')\n",
        "         % (epoch + 1, tr_loss, tr_accuracy,\n",
        "         val_loss, val_accuracy))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 3) for input Tensor(\"dense_input:0\", shape=(None, 3), dtype=float32), but it was called on an input with incompatible shape (20, 100, 3).\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 1.0148 - accuracy: 0.5800\n",
            "Epoch #1\t Training Loss: 1.01\tTraining Accuracy: 0.60\tValidation Loss: 1.01\tValidation Accuracy: 0.58\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.0109 - accuracy: 0.5880\n",
            "Epoch #2\t Training Loss: 1.01\tTraining Accuracy: 0.60\tValidation Loss: 1.01\tValidation Accuracy: 0.59\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 1.0037 - accuracy: 0.6040\n",
            "Epoch #3\t Training Loss: 1.01\tTraining Accuracy: 0.60\tValidation Loss: 1.00\tValidation Accuracy: 0.60\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9938 - accuracy: 0.6120\n",
            "Epoch #4\t Training Loss: 1.00\tTraining Accuracy: 0.60\tValidation Loss: 0.99\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9839 - accuracy: 0.6240\n",
            "Epoch #5\t Training Loss: 0.99\tTraining Accuracy: 0.61\tValidation Loss: 0.98\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9750 - accuracy: 0.6280\n",
            "Epoch #6\t Training Loss: 0.98\tTraining Accuracy: 0.62\tValidation Loss: 0.98\tValidation Accuracy: 0.63\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9677 - accuracy: 0.6320\n",
            "Epoch #7\t Training Loss: 0.97\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.63\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9628 - accuracy: 0.6160\n",
            "Epoch #8\t Training Loss: 0.97\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9595 - accuracy: 0.6200\n",
            "Epoch #9\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9570 - accuracy: 0.6200\n",
            "Epoch #10\t Training Loss: 0.96\tTraining Accuracy: 0.61\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9556 - accuracy: 0.6160\n",
            "Epoch #11\t Training Loss: 0.96\tTraining Accuracy: 0.61\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9552 - accuracy: 0.6080\n",
            "Epoch #12\t Training Loss: 0.95\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9559 - accuracy: 0.6240\n",
            "Epoch #13\t Training Loss: 0.95\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9580 - accuracy: 0.6240\n",
            "Epoch #14\t Training Loss: 0.95\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9613 - accuracy: 0.6120\n",
            "Epoch #15\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9653 - accuracy: 0.6160\n",
            "Epoch #16\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9691 - accuracy: 0.6160\n",
            "Epoch #17\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9721 - accuracy: 0.6120\n",
            "Epoch #18\t Training Loss: 0.97\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9739 - accuracy: 0.6040\n",
            "Epoch #19\t Training Loss: 0.97\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.60\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9741 - accuracy: 0.6040\n",
            "Epoch #20\t Training Loss: 0.97\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.60\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.9734 - accuracy: 0.6080\n",
            "Epoch #21\t Training Loss: 0.97\tTraining Accuracy: 0.61\tValidation Loss: 0.97\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9720 - accuracy: 0.6120\n",
            "Epoch #22\t Training Loss: 0.97\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.9704 - accuracy: 0.6080\n",
            "Epoch #23\t Training Loss: 0.97\tTraining Accuracy: 0.61\tValidation Loss: 0.97\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9689 - accuracy: 0.6040\n",
            "Epoch #24\t Training Loss: 0.97\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.60\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9673 - accuracy: 0.6080\n",
            "Epoch #25\t Training Loss: 0.97\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9658 - accuracy: 0.6120\n",
            "Epoch #26\t Training Loss: 0.97\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9644 - accuracy: 0.6080\n",
            "Epoch #27\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9633 - accuracy: 0.6120\n",
            "Epoch #28\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9626 - accuracy: 0.6200\n",
            "Epoch #29\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9624 - accuracy: 0.6280\n",
            "Epoch #30\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.63\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9625 - accuracy: 0.6280\n",
            "Epoch #31\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.63\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9629 - accuracy: 0.6240\n",
            "Epoch #32\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9634 - accuracy: 0.6240\n",
            "Epoch #33\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9639 - accuracy: 0.6200\n",
            "Epoch #34\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9642 - accuracy: 0.6200\n",
            "Epoch #35\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9646 - accuracy: 0.6160\n",
            "Epoch #36\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9649 - accuracy: 0.6160\n",
            "Epoch #37\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.9652 - accuracy: 0.6160\n",
            "Epoch #38\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.62\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9653 - accuracy: 0.6120\n",
            "Epoch #39\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.9653 - accuracy: 0.6120\n",
            "Epoch #40\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9652 - accuracy: 0.6120\n",
            "Epoch #41\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.97\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9650 - accuracy: 0.6080\n",
            "Epoch #42\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.9647 - accuracy: 0.6080\n",
            "Epoch #43\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9645 - accuracy: 0.6080\n",
            "Epoch #44\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9643 - accuracy: 0.6080\n",
            "Epoch #45\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9640 - accuracy: 0.6080\n",
            "Epoch #46\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9637 - accuracy: 0.6080\n",
            "Epoch #47\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9634 - accuracy: 0.6080\n",
            "Epoch #48\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9632 - accuracy: 0.6080\n",
            "Epoch #49\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n",
            "8/8 [==============================] - 0s 1ms/step - loss: 0.9630 - accuracy: 0.6080\n",
            "Epoch #50\t Training Loss: 0.96\tTraining Accuracy: 0.62\tValidation Loss: 0.96\tValidation Accuracy: 0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OadIT5MEaOJA",
        "colab_type": "text"
      },
      "source": [
        "## Check Performance on Test Set\n",
        "\n",
        "We can use model.predict to output predicted labels on the test set, or model.evaluate to determine test-set accuracy (since we have the labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr55MHYhafFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7629a44-839f-4401-93db-f332e4f387a8"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_data_test, y_labels_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 1ms/step - loss: 1.0438 - accuracy: 0.5720\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}